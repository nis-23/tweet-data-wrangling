{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brief** \n",
    "\n",
    "WeRateDogs is a popular twitter channel that rates people's dogs on a scale of 10.   \n",
    "They are known for their unique rating system where they would regularly rate dogs with a numerator greater than the denominator.\n",
    "\n",
    "The task in this process to wrangle WeRateDogs's twitter archive.\n",
    "\n",
    "**Process**  \n",
    "\n",
    "The whole wrangling process would be carried out in three parts:\n",
    "\n",
    "*Gathering*\n",
    "             \n",
    " This stage includes gathering data from three sources 1. File on hand 2. A remote server 3. Twitter API. \n",
    "\n",
    "*Assessing*\n",
    "\n",
    "  Assessing the gathered data for quality and tidiness issues using both: visual assessment and programmatic assessment.\n",
    "  \n",
    "*Cleaning*  \n",
    "\n",
    "  Cleaning the data for the issues identified using the define, code, test framework.\n",
    "  \n",
    "*Storing*  \n",
    "\n",
    "  Storing the cleaned and tidied data into a csv file and a sqlite database.\n",
    " \n",
    "            \n",
    "                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brief**\n",
    "\n",
    "All the files gathered for the wrangling are stored in a folder **extracted_files**, which is located a sibling directory to the file wrangle_act.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering twitter_archive_enhanced.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The file contains information about WeRateDogs's tweets: like tweet_id, timestamp, text etc.  \n",
    "* The text column of the file was used to extract further information like name, stage of the dog, and the rating.\n",
    "* The twitter_archive_enhanced.csv was downloaded directly from the workspace and is to be treated as file on hand.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering  image_predictions.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This file contains the predictions of a neural network that ran through the tweet images of WeRateDogs. \n",
    "* Since the file was stored on a remote server, it was downloaded using the requests library and stored in\n",
    "  extracted_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering retweet_count and favorite_count from twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Python's tweepy library was used to query twitter's API.   \n",
    "                                                           \n",
    "                                                           \n",
    "* Queried the twitter API by looping through the tweet_id's in twitter_archive_enhanced, stored the json values returned by the API in a text file called tweet_json.txt using the json.dumps method, taking care to append a new line at the end of each iteration.\n",
    "\n",
    "                 \n",
    " * Handled the failed extractions by storing the error message and the corresponding tweet_id in a list called failed extractions.(these tweets were deleted at the time of quering the database)\n",
    "\n",
    "* Read the json stored in tweet_json.txt using json.loads and extracted attributes retweet_count and favorite_count from the resulting dictionary.   \n",
    "\n",
    "\n",
    "\n",
    "* Looped through the failed extractions list to extract the tweet_id's of failed extractions and stored them to a dataframe called deleted_tweets by appending the columns retweet_count and favorite_count - both instantiated to np.nan.\n",
    "\n",
    "\n",
    "* Stored the two attributes along with the corresponding tweet_id and stored it in a dataframe called tweet_attrs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brief** \n",
    "\n",
    "All the extracted pieces of data would be checked for quality and tidiness issues.  \n",
    "\n",
    "*Visual assessment*\n",
    "\n",
    "This stage mainly helped in identifying accuracy and consistency issues. \n",
    "\n",
    "*Programmatic assesment*\n",
    "\n",
    "This stage helped in checking for validity and completness issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visual assesment :  primary check for accuracy and tidiness\n",
    "\n",
    "Individually viewed all the datasets on the jupyter notebook display. In some cases, created specialized views to help in identifying certain issues.  \n",
    "Example: created a view displaying records where the rating numerator was less than the denominator to identify potential issues; as conventially all dogs would be rated higher than 10. \n",
    "\n",
    "Identified all accuracy and tidiness issues using the displays, and some issues with data validity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Programmatic assesment:  primary check for validity and completness\n",
    " \n",
    " \n",
    "Wrote python scripts to detect missing values and for the datatypes of columns which helped in detecting issues of incorrect datatypes and missing values. \n",
    "\n",
    "*missing values*  \n",
    "  \n",
    "  Used pandas' isnull() function, along with seaborne's heatmap function in identifying  missing values. \n",
    "  \n",
    "*incorrect datatypes*  \n",
    "  \n",
    "  Used pandas' dtypes() to list out the data type of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brief**\n",
    "\n",
    "The data cleaning sequence was carried out using the define, code, test framework.\n",
    "\n",
    "The cleaning sequences were carried out to clean for: \n",
    "\n",
    "1. Quality : Issues with contents of the data.\n",
    "2. Tidiness: Issues with the structure of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Use of define, code, and test framework\n",
    "\n",
    "*Define*\n",
    "\n",
    "A short note on the approach used to solve the cleaning task. \n",
    "\n",
    "*Code* \n",
    "\n",
    "The cleaning code.\n",
    "\n",
    "*Test*\n",
    "\n",
    "A description of the test that would prove the succesfull completion of the cleaning task followed by the code that conducts the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brief**\n",
    "\n",
    "The wrangled data would be stored in two ways:\n",
    "\n",
    "* A directory containing all the cleaned files\n",
    "* A sqlite database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating twitter_archive_master**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* twiiter_archive_master is a master dataframe that contains all the data gathered from the project.  \n",
    "  \n",
    "  \n",
    "* The dataset was created by using joining all the frames using tweet_id as the join key while joining the databases.  \n",
    "  \n",
    "  \n",
    "* The join order of the frames was such that the table on the left, had more rows than one on the right. And left join was used which preserved the key order which meant that all the tweet_id's were kept intact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Storing data  to csv files\n",
    "\n",
    "* Created a sub-directory: cleaned_dataset_files, to store all the cleaned datasets.\n",
    "\n",
    "\n",
    "* The master dataframe, and all datasets that were created for the purpose of tidiness are stored in a sub-directory: submission_files that lay inside the cleaned_dataset_directory.\n",
    "\n",
    "\n",
    "* The cleaned data set directory is structured in the following way:\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    cleaned_dataset_files\n",
    "                    \n",
    "                                image_predictions.csv \n",
    "                                submission_files \n",
    "\n",
    "                                              dog_attrs.csv\n",
    "                                              tweet_attrs.csv\n",
    "                                              twitter_archive_enhanced.csv\n",
    "                                              twitter_archive_master.csv\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing data to SQlite database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Created a SQLite database called submissions using the create_engine function from the SQLAlchemy library. \n",
    "  \n",
    "  \n",
    "* Stored the following dataframes as tables in the SQLite database:\n",
    "  \n",
    "         * twitter_archive_master\n",
    "         * twitter_archive_enhanced\n",
    "         * dog_attrs \n",
    "         * tweet_attrs\n",
    "         \n",
    "\n",
    "* The submissions databse is stored in the folder \"Tweet Data Wrangling\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
