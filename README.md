# Tweet Data Wrangling


## Table of Contents

* [Overview](#Overview)
* [Dependancies](#Dependancies)
* [Setup](#Setup)

## Overview

A portfolio project on Data Wrangling. The project uses the tweet archive of WeRateDogs - a popular twitter channel that rates dogs with a unique rating system
to work through the complete data wrangling lifecycle of data gathering, assessment & cleaning, and storage. And using the stored data for analysis and visualization.

A few data points in addition to the archive data were gathered from a remote server and some by quering twitter'a API. The complete dataset was then cleaned for quality and tidiness and finally, stored it in a Sqlite database.      

The project was done as a course requirement on Udacity's Data Analyst to demonstrate data wrangling abilities. The tweet archive data of WeRateDogs was provided in the begining
whereas the structuring and coding of all other data was to be done from scratch.

## Dependancies

### Tools

* Python 3.8
* Jupyter
* Anaconda 3

### Libraries

* pandas
* numpy 
* matplotlib
* seaborn
* sqlalchemy
* requests
* tweepy

## Setup

* Clone the github repository in the desrired directory.
* Create a conda environment and install all the dependencies using conda or pip.
* Activate the environment; navigate to the project directory in the repo, and type jupyter notebook to open up the repo files in a jupyter server.  


